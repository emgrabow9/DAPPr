{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 How to use the AVE-beta\
Emily Grabowski \
04-08-2019\
\
There are two ways to use this tool. The first is to use a general model that has been trained on a variety of languages to make predictions for another dataset. The second is to use one\'92s own data to train a model and make predictions for the rest of the data.\
\
Using the general model:\
1. Place all of the .wav files in a folder that you want to analyze\
2. Navigate in terminal to the folder that the python scripts are in.\
3. Run the line: python AVE-beta-test.py path-to-the-data-folder\
\
The tool will import the spectral information and load the pre-trained model to predict vowel locations. The information will be output as FILENAME.TextGrid in the same folder that the recordings are in. This may overwrite any TextGrid files already present.\
\
To train your own model:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 1. Make two folders, one with training data that has .wav and .TextGrid files (annotated such that V=vowel and X=everything else) and one that has .wav files. \
2. Navigate in terminal to the folder that the python scripts are in.\
3. Run the line: python AVE-beta-train-test.py path-to-the-train-folder path-to-the-test-folder\
\
The tool will train a new model on the training data with annotations and then predict vowel locations  and make TextGrids for the test data. This will save the model as model.p and the scaler as scaler.p. This will overwrite any models previously trained using this method.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
The results from these methods can then be rolled forward into the DAPPr tool to get pitch annotations.\
\
A few notes: \
-This will predict vowels for all of the speech in the recording, regardless of speaker and language. \
\
-There are some python packages that will be required to be installed on the system, including parselmouth.\
\
-This is a beta version, so any feedback as to how to make the system better or more accessible are welcome.}